{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dynamic FAISS Sentence RAG Demo\n",
        "\n",
        "- FAISS index for vector similarity search\n",
        "- Runtime insertion of new sentences\n",
        "- Paragraph query returns top-N most relevant memory sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q faiss-cpu sentence-transformers numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jakemckenna/Code/dungeon-masters-companion/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "SENTENCE_SPLIT_RE = re.compile(r\"(?<=[.!?])\\s+|\\n+\")\n",
        "\n",
        "def split_into_sentences(text: str) -> List[str]:\n",
        "    parts = [p.strip() for p in SENTENCE_SPLIT_RE.split(text or \"\") if p.strip()]\n",
        "    return parts\n",
        "\n",
        "@dataclass\n",
        "class SearchHit:\n",
        "    sentence: str\n",
        "    score: float\n",
        "\n",
        "class DynamicSentenceMemory:\n",
        "    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.index = None\n",
        "        self.sentences: List[str] = []\n",
        "\n",
        "    def _embed(self, texts: List[str]) -> np.ndarray:\n",
        "        vectors = self.model.encode(\n",
        "            texts,\n",
        "            convert_to_numpy=True,\n",
        "            normalize_embeddings=True,\n",
        "        ).astype(np.float32)\n",
        "        return vectors\n",
        "\n",
        "    def add_sentences(self, sentences: List[str]) -> None:\n",
        "        clean = [s.strip() for s in sentences if s and s.strip()]\n",
        "        if not clean:\n",
        "            return\n",
        "\n",
        "        vectors = self._embed(clean)\n",
        "        if self.index is None:\n",
        "            self.index = faiss.IndexFlatIP(vectors.shape[1])\n",
        "        self.index.add(vectors)\n",
        "        self.sentences.extend(clean)\n",
        "\n",
        "    def add_text(self, text: str) -> None:\n",
        "        self.add_sentences(split_into_sentences(text))\n",
        "\n",
        "    def search(self, paragraph: str, top_n: int = 3) -> List[SearchHit]:\n",
        "        if self.index is None or not self.sentences:\n",
        "            return []\n",
        "\n",
        "        query = self._embed([paragraph])\n",
        "        k = min(top_n, len(self.sentences))\n",
        "        scores, idxs = self.index.search(query, k)\n",
        "\n",
        "        hits: List[SearchHit] = []\n",
        "        for score, idx in zip(scores[0], idxs[0]):\n",
        "            if idx < 0:\n",
        "                continue\n",
        "            hits.append(SearchHit(sentence=self.sentences[int(idx)], score=float(score)))\n",
        "        return hits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Indexed sentences: 7\n"
          ]
        }
      ],
      "source": [
        "memory = DynamicSentenceMemory()\n",
        "\n",
        "seed_text = \"\"\"\n",
        "The player stole Mitch's shoes.\n",
        "Mitch keeps spare boots under his bed.\n",
        "A bartender saw muddy prints near the back door.\n",
        "Mitch suspects the player but has no proof.\n",
        "The town square fountain is cracked and leaking.\n",
        "\"\"\"\n",
        "\n",
        "memory.add_text(seed_text)\n",
        "\n",
        "# Runtime insertion: add new facts during play\n",
        "memory.add_sentences([\n",
        "    \"Mitch found one stolen boot beside the docks.\",\n",
        "    \"A guard reported the player carrying footwear at dawn.\",\n",
        "])\n",
        "\n",
        "print(f\"Indexed sentences: {len(memory.sentences)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. score=0.7182 | The player stole Mitch's shoes.\n",
            "2. score=0.6826 | Mitch found one stolen boot beside the docks.\n",
            "3. score=0.6034 | Mitch keeps spare boots under his bed.\n",
            "4. score=0.5353 | Mitch suspects the player but has no proof.\n"
          ]
        }
      ],
      "source": [
        "query_paragraph = \"\"\"\n",
        "Mitch complains that his footwear is missing after a theft,\n",
        "and he says the player may have taken his boots near the harbor.\n",
        "\"\"\"\n",
        "\n",
        "top_n = 4\n",
        "hits = memory.search(query_paragraph, top_n=top_n)\n",
        "\n",
        "for rank, hit in enumerate(hits, start=1):\n",
        "    print(f\"{rank}. score={hit.score:.4f} | {hit.sentence}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The relevance behavior comes from the embedding model + vectors in the index, not manual semantic rules in notebook code."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
